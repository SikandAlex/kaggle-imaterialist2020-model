# EfficientNet on TPU

## About the model and training regime

EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, et. al.

## Cloud TPU Prerequisites

### Setup a Google Cloud project

Follow the instructions at the [Quickstart Guide][quickstart-guide] to get a GCE
VM with access to a Cloud TPU.

[quickstart-guide]: https://cloud.google.com/tpu/docs/quickstart

To run this model, you will need:

* A GCE VM instance with an associated Cloud TPU resource
* A GCS bucket to store your training checkpoints (the "model directory")
* (Optional): The ImageNet training and validation data preprocessed into
  TFRecord format, and stored in GCS.

**Please make sure your TensorFlow version >= 1.13 for both GCE VM and Cloud TPU.**

### Formatting the data

The data is expected to be formatted in TFRecord format, as generated by [this
script][imagenet-download-format-as-tfrecord].

If you do not have ImageNet dataset prepared, you can use a randomly generated
fake dataset to test the model. It is located at
`gs://cloud-tpu-test-datasets/fake_imagenet`.

[imagenet-download-format-as-tfrecord]: https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py

## Training the model

Add the top-level `/models` folder to the Python path with the command

```
export PYTHONPATH="$PYTHONPATH:/path/to/models"
```

Train the model by executing the following command (substituting the appropriate
values):

```
python main.py \
  --tpu=$TPU_NAME \
  --data_dir=$DATA_DIR \
  --model_dir=$MODEL_DIR \
  --train_batch_size=2048 \
  --train_steps=218949
```

`$TPU_NAME` is the name of the TPU node, the same name that appears when you
run `gcloud compute tpus list`, or `ctpu ls`.  (When using the shell
created by `ctpu up`, this argument may not be necessary.)

`$MODEL_DIR` is a GCS location (a URL starting with `gs://` where both the GCE
VM and the associated Cloud TPU have write access, something like `gs://userid-
dev-imagenet-output/model`.  (TensorFlow can't create the bucket; you have to
create it with `gsutil mb <bucket>`.)  This bucket is used to save checkpoints
and the training result, so that the training steps are cumulative when you
reuse the model directory.  If you do 1000 steps, for example, and you reuse the
model directory, on a subsequent run, it will skip the first 1000 steps, because
it picks up where it left off.

`$DATA_DIR` is a GCS location to which both the GCE VM and associated Cloud TPU
have read access, something like `gs://cloud-tpu-test-datasets/fake_imagenet`.
This location is expected to contain files with the prefixes `train-*` and
`validation-*`.  The former pattern is used to match files used for the training
phase, the latter for the evaluation phase.

This will train a EfficientNet model on ImageNet with 2048 batch size on a single
Cloud TPU. With the default flags on everything, the model should train to
above 76% accuracy for the base EfficientNet model.

The training and validation data can also be sourced from Cloud Bigtable:

```
python main.py \
  --tpu=$TPU_NAME \
  --model_dir=$MODEL_DIR \
  --bigtable_project=$PROJECT \
  --bigtable_instance=$INSTANCE \
  --bigtable_table=$TABLE
```

In this case, the `TFExample` records are stored one per row in a Cloud Bigtable
table. Categories of data are arranged by row prefix, and the rows within that
prefix arranged by zero-filled indexes, e.g. `train_0000003892`.)

Note that even when sourcing input data from Cloud Bigtable, `$MODEL_DIR` must
still be a GCS location.

### Project and Zone

If you are not running this script on a GCE VM in the same project and zone as
your Cloud TPU, you will need to add the `--project` and `--zone` flags
specifying the corresponding values for the Cloud TPU you'd like to use.

You can launch TensorBoard (e.g. `tensorboard -logdir=$MODEL_DIR`) to view loss
curves and other metadata regarding your training run. Please follow [tensorboard-setup](https://cloud.google.com/tpu/docs/tensorboard-setup) for detailed steps.

> Note: if you launch TensorBoard on your GCE VM, be sure to configure either
> [SSH port forwarding][ssh-port-fwd] or [SOCKS proxy over SSH][socks-proxy] to
> connect to your GCE VM **securely (recommended)**.
>
> Alternatively, you can modify your GCE firewall rules to open a port, but this
> is **not recommended** as it enables **insecure** world-wide access for
> everyone.

[ssh-port-fwd]: https://cloud.google.com/solutions/connecting-securely#port-forwarding-over-ssh
[socks-proxy]: https://cloud.google.com/solutions/connecting-securely#socks-proxy-over-ssh
